{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import decomposition, pipeline, metrics, grid_search\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declarations\n",
    "stemmer = PorterStemmer()\n",
    "sw=[]\n",
    "s_data = []\n",
    "t_data = []\n",
    "t_queries = []\n",
    "t_labels = []\n",
    "t_labelsf = []\n",
    "#stopwords tweak\n",
    "ML_STOP_WORDS = ['http','www','img','border','color','style','padding','table','font','inch','width','height']\n",
    "ML_STOP_WORDS += list(text.ENGLISH_STOP_WORDS)\n",
    "for stw in ML_STOP_WORDS:\n",
    "    sw.append(\"z\"+str(stw))\n",
    "ML_STOP_WORDS += sw\n",
    "for i in range(len(ML_STOP_WORDS)):\n",
    "    ML_STOP_WORDS[i]=stemmer.stem(ML_STOP_WORDS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ML_TEXT_CLEAN(f2,f3):\n",
    "    if len(f2)<3:\n",
    "        f2=\"feature2null\"\n",
    "    if len(f3)<3:\n",
    "        f3=\"feature3null\"\n",
    "    tx = BeautifulSoup(f3)\n",
    "    tx1 = [x.extract() for x in tx.findAll('script')]\n",
    "    tx = tx.get_text(\" \").strip()\n",
    "    s = (\" \").join([\"z\"+ str(z) for z in f2.split(\" \")]) + \" \" + tx\n",
    "    s = re.sub(\"[^a-zA-Z0-9]\",\" \", s)\n",
    "    s = re.sub(\"[0-9]{1,3}px\",\" \", s)\n",
    "    s = re.sub(\" [0-9]{1,6} |000\",\" \", s)\n",
    "    s = (\" \").join([stemmer.stem(z) for z in s.split(\" \") if len(z)>2])\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "train = pd.read_csv(\"../raw_train.csv\").fillna(\" \")\n",
    "test  = pd.read_csv(\"../raw_test.csv\").fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65497012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65516012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/6552101\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65527\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januarya/14146012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train.id)):\n",
    "    s = ML_TEXT_CLEAN(train.product_title[i], train.product_description[i])\n",
    "    s_data.append((train[\"query\"][i], s, str(train[\"median_relevance\"][i])))\n",
    "\n",
    "for i in range(len(test.id)):\n",
    "    s = ML_TEXT_CLEAN(test.product_title[i], test.product_description[i])\n",
    "    t_data.append((test[\"query\"][i], s, test.id[i]))\n",
    "    if test[\"query\"][i] not in t_queries:\n",
    "        t_queries.append(test[\"query\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(s_data)\n",
    "df2 = pd.DataFrame(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tq in t_queries:\n",
    "    \n",
    "    df1_s = df1[df1[0]==tq]\n",
    "    df2_s = df2[df2[0]==tq]\n",
    "    \n",
    "    #Naive Bayes\n",
    "    clf = MultinomialNB(alpha=0.01)\n",
    "    v = TfidfVectorizer(use_idf=True,min_df=0,ngram_range=(1,6),lowercase=True,sublinear_tf=True,stop_words=ML_STOP_WORDS)\n",
    "    clf.fit(v.fit_transform(df1_s[1]), df1_s[2])\n",
    "    t_labels_nb = clf.predict(v.transform(df2_s[1]))\n",
    "    \n",
    "    #SDG\n",
    "    clf = Pipeline([('v',TfidfVectorizer(max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 6), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = ML_STOP_WORDS)), ('sdg', SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1, penalty='l2', power_t=0.5, random_state=None, shuffle=True, verbose=0, warm_start=False))])\n",
    "    clf.fit(df1_s[1], df1_s[2])\n",
    "    t_labels_sdg = clf.predict(df2_s[1])\n",
    "    \n",
    "    #SVD/Standard Scaler/SVM\n",
    "    clf = Pipeline([('v',TfidfVectorizer(max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 6), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = ML_STOP_WORDS)), ('svd', TruncatedSVD(n_components=100)),  ('scl', StandardScaler()), ('svm', SVC(C=10))])\n",
    "    clf.fit(df1_s[1], df1_s[2])\n",
    "    t_labels_sv_ = clf.predict(df2_s[1])\n",
    "    \n",
    "    #Decision Tree\n",
    "    clf = Pipeline([('v',TfidfVectorizer(max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 6), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = ML_STOP_WORDS)), ('dtc', DecisionTreeClassifier(random_state=0))])\n",
    "    clf.fit(df1_s[1], df1_s[2])\n",
    "    t_labels_dtc = clf.predict(df2_s[1])\n",
    "    \n",
    "    #KNeighbors\n",
    "    clf = Pipeline([('v',TfidfVectorizer(max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 6), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = ML_STOP_WORDS)), ('kn', KNeighborsClassifier(n_neighbors=3))])\n",
    "    clf.fit(df1_s[1], df1_s[2])\n",
    "    t_labels_kn = clf.predict(df2_s[1])\n",
    "    \n",
    "    #print(tq)\n",
    "    for i in range(len(t_labels_nb)):\n",
    "        t_labels1 = list(df2_s[2])\n",
    "        t_labelsf.append((t_labels1[i],t_labels_nb[i],t_labels_sdg[i],t_labels_sv_[i],t_labels_dtc[i],t_labels_kn[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(t_labelsf)\n",
    "df3 = df3.sort([0])\n",
    "preds2 = list(df3[1])\n",
    "preds3 = list(df3[2])\n",
    "preds4 = list(df3[3])\n",
    "preds5 = list(df3[4])\n",
    "preds6 = list(df3[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p3 = []\n",
    "for i in range(len(preds2)):\n",
    "    x = round((int(preds6[i]) + int(preds5[i]) + int(preds4[i]) + int(preds3[i]) + int(preds2[i]))/5,0)\n",
    "    p3.append(int(x))\n",
    "    \n",
    "submission = pd.DataFrame({\"id\": test.id, \"prediction\": p3})\n",
    "submission.to_csv(\"ensemble5models.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
