{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_train_test.csv\")\n",
    "data.loc[:,'text'] = data.apply(\n",
    "    lambda x: \"%s %s\" % (x['query'], x['product_title']),axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tokenization via TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize all of the data (train AND test) in order to be aware of all the terms in both sets. Then, we need to divide this tokenized data into train and test. In order to do this, we need to know the row indices of train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(t.lower()) for t in tokens]\n",
    "\n",
    "tokenization_pipeline = Pipeline(steps = [\n",
    "    ('tfidf',TfidfVectorizer(tokenizer = tokenize, stop_words = 'english'))\n",
    "])\n",
    "\n",
    "train_test_split_idx = data[data.dataset == 'train'].shape[0]\n",
    "X = tokenization_pipeline.fit_transform(data['text'])\n",
    "X_train = X[:train_test_split_idx,:] \n",
    "y_train = data.loc[:train_test_split_idx-1,'median_relevance']\n",
    "X_test =  X[train_test_split_idx:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10158, 23720)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tuning_sample, y_tuning_sample = X_train[:1000,:], y_train[:1000]\n",
    "prediction_pipeline = Pipeline(steps = [\n",
    "    ('PCA',TruncatedSVD()),\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('classifier',LogisticRegression())\n",
    "])\n",
    "\n",
    "grid_search_parameters = {\n",
    "    'PCA__n_components': [100,500,-1],\n",
    "    'classifier__penalty': ['l1','l2'],\n",
    "    'classifier__class_weight': [None,'auto']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best score: 0.3098\n",
      "Best parameters set: {'classifier__class_weight': 'auto', 'classifier__penalty': 'l2', 'PCA__n_components': 500}\n",
      "Model trained in 69.0 seconds\n"
     ]
    }
   ],
   "source": [
    "grid_search_start = time.time()\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "    \n",
    "    kappa_scorer = metrics.make_scorer(\n",
    "        quadratic_weighted_kappa, \n",
    "        greater_is_better = True\n",
    "    )\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator  = prediction_pipeline,\n",
    "        param_grid = grid_search_parameters,\n",
    "        cv = 5,\n",
    "        n_jobs = -1,\n",
    "        verbose = 1,\n",
    "        scoring = kappa_scorer\n",
    "    )\n",
    "    grid_search.fit(X_tuning_sample,y_tuning_sample)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best score: %0.4f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\",best_params)\n",
    "    print(\"Model trained in %0.1f seconds\" % (time.time() - grid_search_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score: 0.4601\n",
      "Training time: 94.9 seconds\n"
     ]
    }
   ],
   "source": [
    "training_start = time.time()\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "    kappa_scorer = metrics.make_scorer(\n",
    "        quadratic_weighted_kappa, \n",
    "        greater_is_better = True\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        estimator = grid_search.best_estimator_,\n",
    "        X = X_train,\n",
    "        y = y_train,\n",
    "        scoring = kappa_scorer,\n",
    "        cv = 10,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "print(\"Average cross validation score: %0.4f\" % scores.mean())\n",
    "print(\"Training time: %0.1f seconds\" % (time.time() - training_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 0.3 seconds\n"
     ]
    }
   ],
   "source": [
    "prediction_start = time.time()\n",
    "results = pd.DataFrame({\n",
    "'id': np.array(data.loc[train_test_split_idx:,'id']),\n",
    "'prediction': grid_search.best_estimator_.predict(X_test)\n",
    "})\n",
    "print(\"Prediction time: %0.1f seconds\" % (time.time() - prediction_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('../submissions/01_logistic_regression_0.46.csv',index=False, float_format = \"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
