{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from ml_metrics import quadratic_weighted_kappa\n",
    "import time\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_train_test.csv\")\n",
    "data.loc[:,'text'] = data.apply(\n",
    "    lambda x: \"%s %s\" % (x['query'], x['product_title']),axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tokenization via TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize all of the data (train AND test) in order to be aware of all the terms in both sets. Then, we need to divide this tokenized data into train and test. In order to do this, we need to know the row indices of train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    return [stemmer.stem(t.lower()) for t in tokens]\n",
    "\n",
    "tokenization_pipeline = Pipeline(steps = [\n",
    "    ('tfidf',TfidfVectorizer(tokenizer = tokenize, stop_words = 'english'))\n",
    "])\n",
    "\n",
    "train_test_split_idx = data[data.dataset == 'train'].shape[0]\n",
    "X = tokenization_pipeline.fit_transform(data['text'])\n",
    "X_train = X[:train_test_split_idx,:] \n",
    "y_train = data.loc[:train_test_split_idx-1,'median_relevance']\n",
    "\n",
    "test_set_ids = np.array(data.loc[train_test_split_idx:,'id'])\n",
    "X_test =  X[train_test_split_idx:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_pipeline = Pipeline(steps = [\n",
    "    ('PCA',TruncatedSVD()),\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('classifier',SVC())\n",
    "])\n",
    "\n",
    "grid_search_parameters = {\n",
    "    'PCA__n_components': [100,500,1000,-1],\n",
    "    'classifier__C': [10e-2,1.0,10e2],\n",
    "    'classifier__gamma': [10e-2,1.0,10e2],\n",
    "    'classifier__class_weight': [None,'auto']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   24.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed: 85.9min\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 216 | elapsed: 86.5min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed: 86.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best score: 0.4642\n",
      "Best parameters set: {'PCA__n_components': 100, 'classifier__C': 1000.0, 'classifier__gamma': 0.1, 'classifier__class_weight': 'auto'}\n",
      "Model trained in 5232.3 seconds\n"
     ]
    }
   ],
   "source": [
    "grid_search_start = time.time()\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "    \n",
    "    kappa_scorer = metrics.make_scorer(\n",
    "        quadratic_weighted_kappa, \n",
    "        greater_is_better = True\n",
    "    )\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator  = prediction_pipeline,\n",
    "        param_grid = grid_search_parameters,\n",
    "        cv = StratifiedKFold(y = y_train,n_folds=3),\n",
    "        n_jobs = -1,\n",
    "        verbose = 1,\n",
    "        scoring = kappa_scorer\n",
    "    )\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best score: %0.4f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\",best_params)\n",
    "    print(\"Model trained in %0.1f seconds\" % (time.time() - grid_search_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation score: 0.4750\n",
      "All Cross Validation Scores: [ 0.51681851  0.45404373  0.48279532  0.51216152  0.4175586   0.47406038\n",
      "  0.47793642  0.48722882  0.43634664  0.49062778]\n",
      "Training time: 126.0 seconds\n"
     ]
    }
   ],
   "source": [
    "training_start = time.time()\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "    kappa_scorer = metrics.make_scorer(\n",
    "        quadratic_weighted_kappa, \n",
    "        greater_is_better = True\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        estimator = grid_search.best_estimator_,\n",
    "        X = X_train,\n",
    "        y = y_train,\n",
    "        scoring = kappa_scorer,\n",
    "        cv = StratifiedKFold(y = y_train,n_folds=10),\n",
    "        n_jobs = -1\n",
    "    )\n",
    "print(\"Average cross validation score: %0.4f\" % scores.mean())\n",
    "print(\"All Cross Validation Scores: %s\" % scores)\n",
    "print(\"Training time: %0.1f seconds\" % (time.time() - training_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 22.6 seconds\n"
     ]
    }
   ],
   "source": [
    "prediction_start = time.time()\n",
    "results = pd.DataFrame({\n",
    "'id': test_set_ids,\n",
    "'prediction': grid_search.best_estimator_.predict(X_test)\n",
    "})\n",
    "print(\"Prediction time: %0.1f seconds\" % (time.time() - prediction_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('../submissions/03_svm.csv',index=False, float_format = \"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
